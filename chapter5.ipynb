{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a556da-e4cd-4d93-a2aa-d7157f239972",
   "metadata": {},
   "source": [
    "# Principal kernel recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd82aa-5726-40e0-8b88-3a495068fafa",
   "metadata": {},
   "source": [
    "We want to explicitly check the validity of Eq. 5.4 of arXiv:2106.10165. To this end we write down a NN with 5 hidden layers, each with nL neurons, with user-specified activation function. Then we extract the value of $K_{00}$ at the 3-rd and 4-th layer and check if they are indeed related by Eq. 5.4 (within error-bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8c8c67bc-5673-4b3e-b638-38f4c7b58905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d4d88-3b3a-4166-ad1e-f4bd1f91823b",
   "metadata": {},
   "source": [
    "Hyperparameters of the network. n_inputs is the number of inputs, not to be confused with n_samples, which is the number of times we initialize the network, i.e. the dimension of the ensemble of networks we use to measure statistics (like means and variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "194aabaa-c846-4a6f-b8fb-0e7d6e94f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "nL = 200\n",
    "cw = 10\n",
    "cb = 0.\n",
    "n_inputs = 100\n",
    "n_samples = 100\n",
    "in_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69029673-619a-425a-9e37-517065c709fb",
   "metadata": {},
   "source": [
    "Network class. Here we have 1 input layer with \"in_size\" inputs and \"nL\" outputs, 5 hidden layers with \"nL\" inputs and \"nL\" outputs, and one output layer with \"nL\" inputs and 1 output. We pick the preactivations z3 and z4 from the 3rd and 4th hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e37d8ee1-c34c-4d08-b7a8-381fba12841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_NN, self).__init__()\n",
    "        self.firstlayer = nn.Linear(in_features=in_size, out_features=nL)\n",
    "        self.layer1 = nn.Linear(nL, nL)\n",
    "        self.layer2 = nn.Linear(nL, nL)\n",
    "        self.layer3 = nn.Linear(nL, nL)\n",
    "        self.layer4 = nn.Linear(nL, nL)\n",
    "        self.layer5 = nn.Linear(nL, nL)\n",
    "        self.finallayer = nn.Linear(in_features=nL, out_features=1)\n",
    "        \n",
    "        # Initialize weights from a Gaussian distribution\n",
    "        init.normal_(self.firstlayer.weight, mean=0.0, std=np.sqrt(cw/n_inputs))\n",
    "        init.normal_(self.layer1.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        init.normal_(self.layer2.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        init.normal_(self.layer3.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        init.normal_(self.layer4.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        init.normal_(self.layer5.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        init.normal_(self.finallayer.weight, mean=0.0, std=np.sqrt(cw/nL))\n",
    "        \n",
    "        # Initialize biases from a Gaussian distribution\n",
    "        init.normal_(self.firstlayer.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.layer1.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.layer2.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.layer3.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.layer4.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.layer5.bias, mean=0.0, std=cb)\n",
    "        init.normal_(self.finallayer.bias, mean=0.0, std=cb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z0 = x\n",
    "        z1 = self.firstlayer(x)\n",
    "        x = F.relu(z1)\n",
    "        z2 = self.layer1(x)\n",
    "        x = F.relu(z2)\n",
    "        z3 = self.layer2(x)\n",
    "        x = F.relu(z3)\n",
    "        z4 = self.layer3(x)\n",
    "        x = F.relu(z4)\n",
    "        z5 = self.layer4(x)\n",
    "        x = F.relu(z5)\n",
    "        z6 = self.layer5(x)\n",
    "        x = F.relu(z6)\n",
    "        x = self.finallayer(x) #the final layer has no Activation function applied to it\n",
    "        return x, z3, z4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7cf836e3-7676-4c59-8a78-f6d2cc7717f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_inputs,in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "89eb1a55-cd4d-40c8-afd0-8ae9d7267fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(k):\n",
    "    x = torch.linspace(-100,100,1000)\n",
    "    norm = torch.sqrt(torch.tensor(2*np.pi*k))\n",
    "    return ((sum((F.relu(x)**2)*np.exp(-x**2/(2*k)))/norm)/1000)*200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1cf17-4e5b-41ea-ac6e-856a427ec51e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Since nL is pretty large, as a first approximation we take the kernel K, i.e. the first order approximation of the metric, to be the full metric. This is correct up to O(1/nL) corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63f0e9-fbed-420c-8c8e-e7417cd6d37f",
   "metadata": {},
   "source": [
    "Without loss of generality we consider the 0-th input, compute the average of the kernel at the 3rd and 4th layer over n_samples initializations of the network, and check if it they are related by eq 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dbf8a5c0-5e22-48d8-bbb5-170c752e42f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9603, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "k3vec = []\n",
    "k4vec = []\n",
    "for _ in range(n_samples):\n",
    "    model = Simple_NN()\n",
    "    xout, z3, z4 = model(x)\n",
    "    k3vec.append(sum((z3[0].detach())**2)/nL)\n",
    "    k4vec.append(sum((z4[0].detach())**2)/nL)\n",
    "\n",
    "k3 = np.mean(k3vec)\n",
    "k4 = np.mean(k4vec)\n",
    "k4exp = cb + cw * g(k3)\n",
    "print(k4/k4exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
